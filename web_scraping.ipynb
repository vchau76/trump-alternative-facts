{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to scrape data from the Washington Post's Fact Check database\n",
    "\n",
    "Script uses Selenium for interactive features on the page and BeautifulSoup for scraping the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('-incognito')\n",
    "chromedriver = '/Users/vchau76/Desktop/Graduate School/FSB/STAT5006/Final Project/chromedriver' \n",
    "driver = webdriver.Chrome(executable_path = chromedriver, options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open URL of page to scrape\n",
    "\n",
    "url = 'https://www.washingtonpost.com/graphics/politics/trump-claims-database/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium for clicking button to load dynamic content\n",
    "\n",
    "from IPython.core.display import clear_output\n",
    "from time import sleep,time\n",
    "timestart_time = time()\n",
    "\n",
    "# Loop through and click 'Load more claims' button using selenium - loads 50 new claims each time (total of 13435 claims)\n",
    "\n",
    "requests = 0\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\"button.pg-button\").click()\n",
    "        \n",
    "        requests += 1\n",
    "        # Set random time to wait before clicking button again\n",
    "        sleep(randint(5,15))\n",
    "        current_time = time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        \n",
    "        # print out each request and frequency\n",
    "        print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        button = driver.find_element_by_css_selector(\"button.pg-button\").text\n",
    "        if 'Load more claims' not in button:\n",
    "            print(\"There are no more claims.\")\n",
    "            break\n",
    "    except NoSuchElementException as error:\n",
    "        print(error)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup for webscraping data\n",
    "\n",
    "html_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# List to append all data values\n",
    "lies = []\n",
    "\n",
    "# Container for each lie with all associated data values we are trying to scrape\n",
    "claims_container = html_soup.find_all('div', class_ ='claim-row')\n",
    "\n",
    "# Loop through each container to grab data values for each lie\n",
    "for container in claims_container:\n",
    "\n",
    "    dates_elem = container.find('span',class_='label').text # date of lie\n",
    "    \n",
    "    analysis_elem = container.find('div',class_='analysis').find('p',class_='pg-bodyCopy').text # Washington Post analysis\n",
    "    fc_rating_count = container.find_all('span',{'class': 'pinocchio'}) # count number of pinocchios\n",
    "    fc_rating_elem = len(fc_rating_count)\n",
    "\n",
    "    # flags for IF statements\n",
    "    repeated_elem_flag = container.find('span', class_='repeated-total') # flag to determine if/when lie was repeated\n",
    "    repeated_dates_flag = container.find('div', class_='repeats') # flag to determine if lies are repeated\n",
    "    no_repeat_flag = container.find('div',class_=\"details not-expanded\") # flag if no repeated dates \n",
    "    lies_elem_flag = container.find('p', class_='pg-bodyCopy has-apos') # lie\n",
    " \n",
    "    if lies_elem_flag:\n",
    "        lies_elem = container.find('p', class_='pg-bodyCopy has-apos').text.strip('“”') # lie\n",
    "        \n",
    "    #checks for repeated instances of lie\n",
    "    if repeated_elem_flag: \n",
    "        repeated_elem = container.find('span', class_='underline--green').text.rstrip('times').strip() # number of times lies repeated\n",
    "    else:\n",
    "        repeated_elem = 0\n",
    "        \n",
    "    if repeated_dates_flag:\n",
    "        rp_dates = container.find_all('span','repeat pg-highlight')\n",
    "        repeated_dates = [dates.text for dates in rp_dates]\n",
    "        repeated_dates = ', '.join(repeated_dates)\n",
    "    if no_repeat_flag:\n",
    "        topic_elem = no_repeat_flag.select_one('p:nth-of-type(1)').text.lstrip('Topic:').strip()\n",
    "        source_elem = no_repeat_flag.select_one('p:nth-of-type(2)').text.lstrip('Source:').strip()\n",
    "    else:\n",
    "        lies_elm = None\n",
    "        repeated_dates = 0\n",
    "         \n",
    "    new = ((dates_elem,int(repeated_elem),repeated_dates,topic_elem,source_elem,lies_elem,analysis_elem,fc_rating_elem))\n",
    "    lies.append(new)\n",
    "    \n",
    "             \n",
    "df = pd.DataFrame(lies, columns=['date','times repeated','dates repeated','topic','source','lies','analysis','fact check rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%b %d %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV file\n",
    "df_export = df.to_csv('DT_lies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the Selenium browser session\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
